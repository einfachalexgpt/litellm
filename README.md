<h1 align="center"> 
    üöÖ LiteLLM 
</h1> 

<p align="center"> 
    <a href="https://render.com/deploy?repo=https://github.com/BerriAI/litellm" target="_blank" rel="nofollow"><img src="https://render.com/images/deploy-to-render-button.svg" alt="Zum Rendern bereitstellen"></a> 
    <a href="https://railway.app/template/HLP0Ub?referralCode=jch2ME"> 
      <img src="https://railway.app/button.svg" alt="Auf der Eisenbahn einsetzen"> 
    </a> 
</p>

<p align="center">Rufen Sie alle LLM-APIs im OpenAI-Format auf [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq usw.]<br></p> 

<h4 align="center"><a href="https://docs.litellm.ai/docs/simple_proxy" target="_blank">LiteLLM Proxy Server (LLM Gateway)</a> | <a href="https://docs.litellm.ai/docs/hosted" target="_blank">Gehosteter Proxy (Vorschau)</a> | <a href="https://docs.litellm.ai/docs/enterprise" target="_blank">Enterprise Tier</a></h4> 

<h4 align="center"> 
    <a href="https://pypi.org/project/litellm/" target="_blank"> 
        <img src="https://img.shields.io/pypi/v/litellm.svg" alt="PyPI-Version"> 
    </a> 
    <a href="https://dl.circleci.com/status-badge/redirect/gh/BerriAI/litellm/tree/main" target="_blank"> 
        <img src="https://dl.circleci.com/status-badge/img/gh/BerriAI/litellm/tree/main.svg?style=svg" alt="CircleCI"> 
    </a> 
    <a href="https://www.ycombinator.com/companies/berriai"> 
        <img src="https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square" alt="Y Combinator W23"> 
    </a> 
    <a href="https://wa.link/huol9n"> 
        <img src="https://img.shields.io/static/v1?label=Chat%20on&message=WhatsApp&color=success&logo=WhatsApp&style=flat-square" alt="WhatsApp"> 
    </a> 
    <a href="https://discord.gg/wuPM9dRgDw"> 
        <img src="https://img.shields.io/static/v1?label=Chat%20on&message=Discord&color=blue&logo=Discord&style=flat-square" alt="Discord"> 
    </a> 
</h4> 

LiteLLM verwaltet:

- √úbersetzen von Eingaben in die Endpunkte "Completion", "Embedding" und "image_generation" des Anbieters 
- [Konsistente Ausgabe](https://docs.litellm.ai/docs/completion/output), Textantworten sind immer unter `['choices'][0]['message']['content']` verf√ºgbar. 
- Wiederholungs-/Fallback-Logik √ºber mehrere Bereitstellungen hinweg (z.‚ÄØB. Azure/OpenAI) ‚Äì [Router](https://docs.litellm.ai/docs/routing) 
- Festlegen von Budgets und Ratenlimits pro Projekt, API-Schl√ºssel, Modell [LiteLLM Proxy Server (LLM Gateway)](https://docs.litellm.ai/docs/simple_proxy) 

[**Springen Sie zu den Dokumenten zum LiteLLM-Proxy (LLM-Gateway)**](https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs)<br> 
[**Springen Sie zu Unterst√ºtzte LLM-Anbieter**](https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs) 

üö® **Stabile Version:** Verwenden Sie Docker-Images mit dem '-stable'-Tag. Diese wurden 12-st√ºndigen Lasttests unterzogen, bevor sie ver√∂ffentlicht wurden.  

Unterst√ºtzung f√ºr mehr Anbieter. Wenn ein Anbieter oder eine LLM-Plattform fehlt, stellen Sie eine [Featureanforderung](https://github.com/BerriAI/litellm/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.yml&title=%5BFeature%5D%3A+). 

# Nutzung ([**Docs**](https://docs.litellm.ai/docs/)) 

> **WICHTIG**  
> LiteLLM v1.0.0 ben√∂tigt jetzt `openai>=1.0.0`. Leitfaden f√ºr die Migration [hier](https://docs.litellm.ai/docs/migration)  
> LiteLLM v1.40.14+ ben√∂tigt nun `pydantic>=2.0.0`. Es sind keine √Ñnderungen erforderlich. 

[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb) 

```shell
pip install litellm 
``` 

```python
from litellm import completion 
import os 

## ENV-Variablen setzen 
os.environ["OPENAI_API_KEY"] = "Ihr-openai-Schl√ºssel" 
os.environ["COHERE_API_KEY"] = "Ihr-cohere-Schl√ºssel" 

messages = [{ "content": "Hallo, wie geht es Ihnen?", "role": "user"}] 

# OpenAI-Aufruf 
response = completion(model="gpt-3.5-turbo", messages=messages) 

# Cohere-Aufruf 
response = completion(model="command-nightly", messages=messages) 
print(response) 
``` 

Rufen Sie ein beliebiges Modell, das von einem Anbieter unterst√ºtzt wird, mit `model=<provider_name>/<model_name>` auf. Hier kann es anbieterspezifische Details geben, also lesen Sie [Anbieterdokumentation f√ºr weitere Informationen](https://docs.litellm.ai/docs/providers) 

## Asynchron ([Dokumente](https://docs.litellm.ai/docs/completion/stream#async-completion)) 

```python
from litellm import acompletion 
import asyncio 

async def test_get_response(): 
    user_message = "Hallo, wie geht es dir?" 
    messages = [{"content": user_message, "role": "user"}] 
    response = await acompletion(model="gpt-3.5-turbo", messages=messages) 
    return response 

response = asyncio.run(test_get_response()) 
print(response) 
``` 

## Streaming ([Dokumente](https://docs.litellm.ai/docs/completion/stream)) 

LiteLLM unterst√ºtzt das Zur√ºckstreamen der Modellantwort, √ºbergeben Sie `stream=True`, um einen Streaming-Iterator als Antwort zu erhalten.   
Streaming wird f√ºr alle Modelle unterst√ºtzt (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.) 

```python
from litellm import completion 

response = completion(model="gpt-3.5-turbo", messages=messages, stream=True) 
for part in response: 
    print(part.choices[0].delta.content or "") 

# Claude 2 
response = completion('claude-2', messages, stream=True) 
for part in response: 
    print(part.choices[0].delta.content or "") 
``` 

## Beobachtbarkeit der Protokollierung ([Dokumente](https://docs.litellm.ai/docs/observability/callbacks)) 

LiteLLM stellt vordefinierte Callbacks zur Verf√ºgung, um Daten an Lunary, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack zu senden 

```python
from litellm import completion 
import os

## ENV-Variablen f√ºr Logging-Tools setzen 
os.environ["LUNARY_PUBLIC_KEY"] = "Ihr-lunary-public_key" 
os.environ["HELICONE_API_KEY"] = "Ihr-helicone-auth-key" 
os.environ["LANGFUSE_PUBLIC_KEY"] = "" 
os.environ["LANGFUSE_SECRET_KEY"] = "" 
os.environ["ATHINA_API_KEY"] = "Ihr-athina-api-key" 

os.environ["OPENAI_API_KEY"] = "Ihr-openai-Schl√ºssel"

# Callbacks setzen 
litellm.success_callback = ["lunary", "langfuse", "athina", "helicone"] # log Ein-/Ausgabe an lunary, langfuse, supabase, athina, helicone usw. 

# OpenAI Anruf 
response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hallo üëã - ich bin openai"}]) 
``` 

# LiteLLM Proxy Server (LLM Gateway) - ([Dokumente](https://docs.litellm.ai/docs/simple_proxy)) 

Verfolgen Sie Ausgaben + Load Balance √ºber mehrere Projekte hinweg 

[Gehosteter Proxy (Vorschau)](https://docs.litellm.ai/docs/hosted) 

Der Proxy bietet Folgendes: 

1. [Hooks f√ºr die Authentifizierung](https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth) 
2. [Hooks f√ºr die Protokollierung](https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class) 
3. [Kostenverfolgung](https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend) 
4. [Ratenbegrenzung](https://docs.litellm.ai/docs/proxy/users#set-rate-limits) 

## üìñ Proxy-Endpunkte - [Swagger Docs](https://litellm-api.up.railway.app/) 

## Schnellstart-Proxy - CLI 

```shell
pip install 'litellm[Proxy]' 
``` 

### Schritt 1: LiteLLM Proxy starten 

```shell
litellm --model huggingface/bigcode/starcoder 

#INFO: Proxy wird auf http://0.0.0.0:4000 ausgef√ºhrt 
``` 

### Schritt 2: Stellen Sie eine ChatCompletions-Anfrage an den Proxy 

> **WICHTIG**  
> üí° [Verwenden Sie den LiteLLM-Proxy mit Langchain (Python, JS), OpenAI SDK (Python, JS), Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl](https://docs.litellm.ai/docs/proxy/user_keys)  

```python
import openai  # OpenAI v1.0.0+

client = openai.OpenAI(api_key="anything", base_url="http://0.0.0.0:4000")  # Proxy auf base_url setzen

# Anfrage an das Model-Set auf litellm proxy gesendet, 'litellm --model'
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Das ist eine Testaufforderung, schreibe ein kurzes Gedicht"
        }
    ]
)

print(response)
``` 

## Verwaltung von Proxy-Schl√ºsseln ([Dokumente](https://docs.litellm.ai/docs/proxy/virtual_keys)) 

Verbinden Sie den Proxy mit einer Postgres-DB, um Proxy-Schl√ºssel zu erstellen 

```shell
# Holen Sie sich den Code 
git clone https://github.com/BerriAI/litellm 

# Zum Ordner gehen 
cd litellm 

# F√ºgen Sie den Master Key hinzu - Sie k√∂nnen dies nach der Einrichtung √§ndern 
echo 'LITELLM_MASTER_KEY="SK-1234"' > .env 

# F√ºgen Sie den LiteLLM Salt-Schl√ºssel hinzu - Sie k√∂nnen dies nach dem Hinzuf√ºgen eines Modells nicht mehr √§ndern 
# Er wird verwendet, um Ihre LLM-API-Schl√ºsseldaten zu verschl√ºsseln / zu entschl√ºsseln 
# Wir empfehlen den [Passwort-Generator](https://1password.com/password-generator/), um einen zuf√§lligen Hash f√ºr den LiteLLM Salt-Schl√ºssel zu erhalten 
echo 'LITELLM_SALT_KEY="SK-1234"' >> .env 

source .env 

# Starten 
docker-compose up 
``` 

Benutzeroberfl√§che auf `/ui` auf Ihrem Proxyserver 

![ui_3](https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033) 

Legen Sie Budgets und Ratenlimits f√ºr mehrere Projekte fest  
`POST /key/generate` 

### Anfrage 

```shell
curl 'http://0.0.0.0:4000/key/generate' \
--header 'Authorization: Bearer sk-1234' \
--header 'Content-Type: application/json' \
--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4", "claude-2"], "duration": "20m", "metadata": {"user": "ishaan@berri.ai", "team": "core-infra"}}'
``` 

### Erwartete Antwort 

```shell
{ 
    "key": "sk-kdExB7Sfa", # Bearer Token 
    "expires": "2023-11-19T01:38:25.838000+00:00" # datetime-Objekt 
} 
``` 

## Unterst√ºtzte Anbieter ([Docs](https://docs.litellm.ai/docs/providers)) 

| Anbieter | [Completion](https://docs.litellm.ai/docs/#basic-usage) | [Streaming](https://docs.litellm.ai/docs/completion/stream#streaming-responses) | [Asynchrone Completion](https://docs.litellm.ai/docs/completion/stream#async-completion) | [Asynchrones Streaming](https://docs.litellm.ai/docs/completion/stream#async-streaming) | [Asynchrone Embeddings](https://docs.litellm.ai/docs/embedding/supported_embedding) | [Asynchrone Bilderzeugung](https://docs.litellm.ai/docs/image_generation) | 
|-------------------------------------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------| 
| [openai](https://docs.litellm.ai/docs/providers/openai) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | 
| [Azure](https://docs.litellm.ai/docs/providers/azure) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | 
| [aws - Sagemaker](https://docs.litellm.ai/docs/providers/aws_sagemaker) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  | 
| [aws - Bedrock](https://docs.litellm.ai/docs/providers/bedrock) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  | 
| [google - vertex_ai](https://docs.litellm.ai/docs/providers/vertex) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | 
| [google - Palm](https://docs.litellm.ai/docs/providers/palm) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [google AI Studio - Gemini](https://docs.litellm.ai/docs/providers/gemini) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [Mistral AI API](https://docs.litellm.ai/docs/providers/mistral) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  | 
| [Cloudflare AI Workers](https://docs.litellm.ai/docs/providers/cloudflare_workers) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [cohere](https://docs.litellm.ai/docs/providers/cohere) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  | 
| [anthropic](https://docs.litellm.ai/docs/providers/anthropic) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [empower](https://docs.litellm.ai/docs/providers/empower) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [huggingface](https://docs.litellm.ai/docs/providers/huggingface) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  | 
| [replicate](https://docs.litellm.ai/docs/providers/replicate) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [together_ai](https://docs.litellm.ai/docs/providers/togetherai) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [OpenRouter](https://docs.litellm.ai/docs/providers/openrouter) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [ai21](https://docs.litellm.ai/docs/providers/ai21) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [baseten](https://docs.litellm.ai/docs/providers/baseten) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [vllm](https://docs.litellm.ai/docs/providers/vllm) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [nlp_cloud](https://docs.litellm.ai/docs/providers/nlp_cloud) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [Aleph Alpha](https://docs.litellm.ai/docs/providers/aleph_alpha) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [Petals](https://docs.litellm.ai/docs/providers/petals) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [Ollama](https://docs.litellm.ai/docs/providers/ollama) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  | 
| [deepinfra](https://docs.litellm.ai/docs/providers/deepinfra) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [Perplexity AI](https://docs.litellm.ai/docs/providers/perplexity) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [Groq AI](https://docs.litellm.ai/docs/providers/groq) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [Deepseek](https://docs.litellm.ai/docs/providers/deepseek) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [anyscale](https://docs.litellm.ai/docs/providers/anyscale) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 
| [IBM - watsonx.ai](https://docs.litellm.ai/docs/providers/watsonx) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  | 
| [Voyage AI](https://docs.litellm.ai/docs/providers/voyage) |  |  |  |  | ‚úÖ |  | 
| [xinference [Xorbits Inference]](https://docs.litellm.ai/docs/providers/xinference) |  |  |  |  | ‚úÖ |  | 
| [Friendliai](https://docs.litellm.ai/docs/providers/friendliai) | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |  |  | 

[**Lesen Sie die Dokumente**](https://docs.litellm.ai/docs/) 

## Beitragen 

So tragen Sie bei: Klonen Sie das Repository lokal ‚Üí Nehmen Sie eine √Ñnderung vor ‚Üí √úbermitteln Sie eine PR mit der √Ñnderung. 

So √§ndern Sie das Repository lokal: 

**Schritt 1:** Klonen des Repositorys 

```shell
git clone https://github.com/BerriAI/litellm.git 
``` 

**Schritt 2:** Navigieren Sie in das Projekt, und installieren Sie Abh√§ngigkeiten: 

```shell
cd litellm 
poetry install -E extra_proxy -E proxy 
``` 

**Schritt 3:** Testen Sie Ihre √Ñnderung: 

```shell
cd litellm/tests  # pwd: docs/litellm/litellm/tests 
poetry run flake8 
poetry run pytest . 
``` 

**Schritt 4:** Reichen Sie eine PR mit Ihren √Ñnderungen ein! üöÄ 

- Pushen Sie Ihren Fork in Ihr GitHub-Repository 
- Von dort aus eine PR einreichen 

# Unternehmen 

F√ºr Unternehmen, die mehr Sicherheit, Benutzerverwaltung und professionellen Support ben√∂tigen 

[Sprechen Sie mit Gr√ºndern](https://calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat) 

Dies umfasst:  
- ‚úÖ **Funktionen unter der [LiteLLM Commercial License](https://docs.litellm.ai/docs/proxy/enterprise)** 
- ‚úÖ **Priorisierung von Funktionen** 
- ‚úÖ **Benutzerdefinierte Integrationen** 
- ‚úÖ **Professioneller Support ‚Äì Dedizierter Discord + Slack** 
- ‚úÖ **Benutzerdefinierte SLAs** 
- ‚úÖ **Sicherer Zugriff mit Single Sign-On** 

# Unterst√ºtzung / Gespr√§ch mit Gr√ºndern 

- [Demo planen üëã](https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version) 
- [Community Discord üí≠ ](https://discord.gg/wuPM9dRgDw) 
- Unsere Nummern üìû +1 (770) 8783-106 / ‚Ä≠+1 (412) 618-6238‚Ä¨ 
- Unsere E-Mails ‚úâÔ∏è ishaan@berri.ai / krrish@berri.ai 

# Warum haben wir das entwickelt? 

- **Bed√ºrfnis nach Einfachheit**: Unser Code wurde extrem kompliziert, wenn es darum ging, Aufrufe zwischen Azure, OpenAI und Cohere zu verwalten und zu √ºbersetzen. 

# Mitwirkende 

<!-- ALL-CONTRIBUTORS-LIST:START - Diesen Abschnitt nicht entfernen oder √§ndern --> 
<!-- prettier-ignore-start --> 
<!-- markdownlint-disable --> 

<!-- markdownlint-restore --> 
<!-- prettier-ignore-end --> 

<!-- ALL-CONTRIBUTORS-LIST:END --> 

<a href="https://github.com/BerriAI/litellm/graphs/contributors"> 
  <img src="https://contrib.rocks/image?repo=BerriAI/litellm" /> 
</a> 
